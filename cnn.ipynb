{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "IMAGE_SIZE = 64\n",
    "SEED = 42\n",
    "AUTOENCODER_EPOCHS = 3\n",
    "CLASSIFIER_EPOCHS = 20\n",
    "num_classes = 5 \n",
    "class_names = [\"cardboard\", \"glass\", \"metal\", \"paper\", \"plastic\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bgd45\\Desktop\\Data mining project\\data_mining_venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bgd45\\Desktop\\Data mining project\\data_mining_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dense, Rescaling\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import tensorflow_datasets as tfds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2390 files belonging to 5 classes.\n",
      "Using 1912 files for training.\n",
      "Using 478 files for validation.\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = \"./dataset\"\n",
    "train, validate = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    shuffle=True,\n",
    "    seed=SEED,\n",
    "    subset=\"both\",\n",
    "    validation_split=0.2,\n",
    "    interpolation='bilinear',\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bgd45\\Desktop\\Data mining project\\data_mining_venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\bgd45\\Desktop\\Data mining project\\data_mining_venv\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\bgd45\\Desktop\\Data mining project\\data_mining_venv\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:From c:\\Users\\bgd45\\Desktop\\Data mining project\\data_mining_venv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "239/239 [==============================] - 29s 117ms/step - loss: 0.0105\n",
      "Epoch 2/3\n",
      "239/239 [==============================] - 28s 118ms/step - loss: 0.0038\n",
      "Epoch 3/3\n",
      "239/239 [==============================] - 29s 120ms/step - loss: 0.0031\n",
      "239/239 [==============================] - 9s 38ms/step\n",
      "Encoded Data Shape: (1912, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "def change_inputs(images, labels):\n",
    "    x = tf.image.resize(normalization_layer(images), [IMAGE_SIZE, IMAGE_SIZE], method=tf.image.ResizeMethod.GAUSSIAN)\n",
    "    return x, x\n",
    "\n",
    "normalized_ds = train.map(change_inputs)\n",
    "\n",
    "# Autoencoder Model\n",
    "def create_autoencoder_model():\n",
    "    input_layer = tf.keras.layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    x = tf.keras.layers.Dropout(0.5)(encoded) \n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder\n",
    "\n",
    "# Training Autoencoder\n",
    "autoencoder_model = create_autoencoder_model()\n",
    "history_autoencoder = autoencoder_model.fit(normalized_ds, epochs=AUTOENCODER_EPOCHS)\n",
    "\n",
    "# Encode data using the trained autoencoder\n",
    "encoded_data = autoencoder_model.predict(normalized_ds)\n",
    "print(f\"Encoded Data Shape: {encoded_data.shape}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From c:\\Users\\bgd45\\Desktop\\Data mining project\\data_mining_venv\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "239/239 [==============================] - 45s 183ms/step - loss: 55.7109 - accuracy: 0.2615 - val_loss: 1.6079 - val_accuracy: 0.2510\n",
      "Epoch 2/20\n",
      "239/239 [==============================] - 47s 196ms/step - loss: 1.5239 - accuracy: 0.3405 - val_loss: 1.6029 - val_accuracy: 0.2552\n",
      "Epoch 3/20\n",
      "239/239 [==============================] - 53s 222ms/step - loss: 1.4705 - accuracy: 0.3876 - val_loss: 1.5826 - val_accuracy: 0.3013\n",
      "Epoch 4/20\n",
      "214/239 [=========================>....] - ETA: 5s - loss: 1.4963 - accuracy: 0.3668"
     ]
    }
   ],
   "source": [
    "def create_classifier_model(autoencoder_model):\n",
    "    # Use the encoder part of the autoencoder as a feature extractor\n",
    "    encoder_output = autoencoder_model.layers[7].output  # Assuming the encoder output is at index 7, adjust if needed\n",
    "\n",
    "    # Add classification layers on top of the encoder output\n",
    "    x = layers.Flatten()(encoder_output)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    output_layer = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    classifier_model = Model(autoencoder_model.input, output_layer)\n",
    "    classifier_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return classifier_model\n",
    "\n",
    "# Pass your autoencoder model when creating the classifier\n",
    "classifier_model = create_classifier_model(autoencoder_model)\n",
    "history_classifier = classifier_model.fit(train, epochs=CLASSIFIER_EPOCHS, validation_data=validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_normalized = validate.map(change_inputs)\n",
    "validate_list = list(validate_normalized.as_numpy_iterator())\n",
    "images_and_labels = list(validate.as_numpy_iterator())\n",
    "# Predict labels for validation images\n",
    "predictions = classifier_model.predict(validate)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "print(predicted_labels)\n",
    "y = np.concatenate([y for x, y in validate], axis=0)\n",
    "\n",
    "# Display a few validation images along with their true and predicted labels\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(15):  # Adjust the number of images you want to display\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    \n",
    "    # Access the batch of images and labels\n",
    "    image_batch = images_and_labels[i][0]\n",
    "    label_batch = images_and_labels[i][1]\n",
    "    \n",
    "    # Access the ith image and label from the batch\n",
    "    image = image_batch[i]\n",
    "    label = label_batch[i]\n",
    "    \n",
    "    plt.imshow(image.astype(np.uint8) / 255)\n",
    "    plt.title(f\"True: {class_names[label]}\\nPredicted: {class_names[predicted_labels[i]]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_validate = validate.map(change_inputs)\n",
    "evaluation_results = autoencoder.evaluate(normalized_validate)\n",
    "print(\"Validation Loss:\", evaluation_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_batch(dataset, batch_size=5):\n",
    "    for images, _ in dataset.take(1):\n",
    "        return images[:batch_size]\n",
    "\n",
    "# Get a batch of images\n",
    "sample_images = get_validation_batch(normalized_validate)\n",
    "\n",
    "# Generate reconstructions\n",
    "reconstructed_images = autoencoder.predict(sample_images)\n",
    "\n",
    "# Visualize original vs. reconstructed images\n",
    "for i in range(len(sample_images)):\n",
    "    plt.subplot(2, len(sample_images), i + 1)\n",
    "    plt.imshow(sample_images[i])\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(2, len(sample_images), i + 1 + len(sample_images))\n",
    "    plt.imshow(reconstructed_images[i])\n",
    "    plt.title(\"Recon\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_mining_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
