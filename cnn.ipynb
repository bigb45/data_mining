{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 24\n",
    "IMAGE_SIZE = 64\n",
    "SEED = 42\n",
    "AUTOENCODER_EPOCHS = 30\n",
    "CLASSIFIER_EPOCHS = 10\n",
    "num_classes = 5 \n",
    "class_names = [\"cardboard\", \"glass\", \"metal\", \"paper\", \"plastic\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dense, Rescaling\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import tensorflow_datasets as tfds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2390 files belonging to 5 classes.\n",
      "Using 1912 files for training.\n",
      "Using 478 files for validation.\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = \"./dataset\"\n",
    "train, validate = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    shuffle=True,\n",
    "    seed=SEED,\n",
    "    subset=\"both\",\n",
    "    validation_split=0.2,\n",
    "    interpolation='bilinear',\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60/60 [==============================] - 30s 467ms/step - loss: 4608.1685\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - 29s 479ms/step - loss: 669.3286\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - 29s 477ms/step - loss: 459.4630\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - 29s 480ms/step - loss: 392.5450\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - 30s 490ms/step - loss: 356.9963\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - 29s 484ms/step - loss: 328.2575\n",
      "Epoch 7/10\n",
      "60/60 [==============================] - 29s 482ms/step - loss: 319.2357\n",
      "Epoch 8/10\n",
      "60/60 [==============================] - 29s 482ms/step - loss: 303.3122\n",
      "Epoch 9/10\n",
      "60/60 [==============================] - 29s 481ms/step - loss: 295.0629\n",
      "Epoch 10/10\n",
      "60/60 [==============================] - 29s 478ms/step - loss: 293.2693\n",
      "60/60 [==============================] - 9s 142ms/step\n",
      "Encoded Data Shape: (1912, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "flip_layer = tf.keras.layers.RandomFlip(\"horizontal_and_vertical\")\n",
    "rotation_layer = tf.keras.layers.RandomRotation(0.2)\n",
    "def change_inputs(images, labels):\n",
    "    x = flip_layer(images)\n",
    "    x = rotation_layer(x)\n",
    "\n",
    "    x = tf.image.resize(x, [IMAGE_SIZE, IMAGE_SIZE], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    return x, x\n",
    "\n",
    "normalized_ds = train.map(change_inputs)\n",
    "\n",
    "# Autoencoder Model\n",
    "def create_autoencoder_model():\n",
    "    input_layer = tf.keras.layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    x = tf.keras.layers.Dropout(0.5)(encoded) \n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    # x = tf.keras.layers.Dropout(0.5)(encoded) \n",
    "    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse', )\n",
    "    return autoencoder\n",
    "\n",
    "# Training Autoencoder\n",
    "autoencoder_model = create_autoencoder_model()\n",
    "history_autoencoder = autoencoder_model.fit(normalized_ds, epochs=AUTOENCODER_EPOCHS)\n",
    "\n",
    "# Encode data using the trained autoencoder\n",
    "encoded_data = autoencoder_model.predict(normalized_ds)\n",
    "print(f\"Encoded Data Shape: {encoded_data.shape}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier_model(autoencoder_model):\n",
    "    # Use the encoder part of the autoencoder as a feature extractor\n",
    "    encoder_output = autoencoder_model.layers[7].output  # Assuming the encoder output is at index 7, adjust if needed\n",
    "\n",
    "    # Add classification layers on top of the encoder output\n",
    "    x = layers.Flatten()(encoder_output)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    output_layer = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    classifier_model = Model(autoencoder_model.input, output_layer)\n",
    "    classifier_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return classifier_model\n",
    "\n",
    "# Pass your autoencoder model when creating the classifier\n",
    "classifier_model = create_classifier_model(autoencoder_model)\n",
    "history_classifier = classifier_model.fit(train, epochs=CLASSIFIER_EPOCHS, validation_data=validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_normalized = validate.map(change_inputs)\n",
    "validate_list = list(validate_normalized.as_numpy_iterator())\n",
    "images_and_labels = list(validate.as_numpy_iterator())\n",
    "# Predict labels for validation images\n",
    "predictions = classifier_model.predict(validate)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "print(predicted_labels)\n",
    "y = np.concatenate([y for x, y in validate], axis=0)\n",
    "\n",
    "# Display a few validation images along with their true and predicted labels\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(4):  # Adjust the number of images you want to display\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    \n",
    "    # Access the batch of images and labels\n",
    "    image_batch = images_and_labels[i][0]\n",
    "    label_batch = images_and_labels[i][1]\n",
    "    \n",
    "    # Access the ith image and label from the batch\n",
    "    image = image_batch[i]\n",
    "    label = label_batch[i]\n",
    "    \n",
    "    plt.imshow(image.astype(np.uint8) / 255)\n",
    "    plt.title(f\"True: {class_names[label]}\\nPredicted: {class_names[predicted_labels[i]]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_validate = validate.map(change_inputs)\n",
    "evaluation_results = autoencoder.evaluate(normalized_validate)\n",
    "print(\"Validation Loss:\", evaluation_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_batch(dataset, batch_size=5):\n",
    "    for images, _ in dataset.take(1):\n",
    "        return images[:batch_size]\n",
    "\n",
    "# Get a batch of images\n",
    "sample_images = get_validation_batch(normalized_validate)\n",
    "\n",
    "# Generate reconstructions\n",
    "reconstructed_images = autoencoder.predict(sample_images)\n",
    "\n",
    "# Visualize original vs. reconstructed images\n",
    "for i in range(len(sample_images)):\n",
    "    plt.subplot(2, len(sample_images), i + 1)\n",
    "    plt.imshow(sample_images[i])\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(2, len(sample_images), i + 1 + len(sample_images))\n",
    "    plt.imshow(reconstructed_images[i])\n",
    "    plt.title(\"Recon\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_mining_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
